#!/usr/bin/env node

// I am a wrapper around basic elastcidump
// I will source all indices from your elasticserach server and dump them out to .json and .mapping.json files
// I probably only work on *nix hosts
// dump --input must be a URL and --output must be a path on this system
// index --input must be a a path on this system --output must be URL

const argv = require('minimist')(process.argv)
const fs = require('fs')
const os = require('os')
const path = require('path')
const async = require('async')
const request = require('request')
const { fork } = require('child_process')
const url = require('url')
const _ = require('lodash')
const ArgParser = require(path.join(__dirname, '..', 'lib', 'argv.js'))
const addAuth = require(path.join(__dirname, '..', 'lib', 'add-auth.js'))

const options = {}
let matchedIndexes = []
let working = 0
let complete = 0
let indexCounter = 0
let workTimeout

const defaults = {
  debug: true,
  parallel: os.cpus().length,
  match: '^.*$',
  input: null,
  output: null,
  scrollId: null,
  scrollTime: '10m',
  timeout: null,
  limit: 100,
  offset: 0,
  direction: 'dump', // default to dump
  'support-big-int': false,
  'big-int-fields': '',
  ignoreAnalyzer: true,
  ignoreChildError: false,
  ignoreData: false,
  ignoreMapping: false,
  ignoreSettings: false,
  ignoreTemplate: false,
  ignoreAlias: true,
  ignoreType: [],
  includeType: null,
  interval: 1000,
  prefix: '',
  suffix: '',
  transform: null,
  headers: null,
  searchBody: null,
  searchWithTemplate: null,
  cert: null,
  key: null,
  pass: null,
  ca: null,
  tlsAuth: false,
  'input-cert': null,
  'input-key': null,
  'input-pass': null,
  'input-ca': null,
  'output-cert': null,
  'output-key': null,
  'output-pass': null,
  'output-ca': null,
  httpAuthFile: null,
  concurrency: 1,
  carryoverConcurrencyCount: true,
  intervalCap: 5,
  concurrencyInterval: 5000,
  overwrite: false,
  fsCompress: false,
  quiet: false
}

const args = new ArgParser({ options })
args.parse(argv, defaults)

_.split(options.ignoreType, ',').forEach(field => {
  const key = `ignore${_.upperFirst(field)}`
  if (_.has(options, key)) {
    options[key] = true
  }
})

if (options.includeType) {
  const ignoreKeys = _.keys(options).filter((key) => key.startsWith('ignore'))
  const includedKeys = _.split(options.includeType, ',')
  ignoreKeys.forEach(k => {
    options[k] = _.indexOf(includedKeys, _.toLower(/^ignore(.*)$/.exec(k)[1])) <= -1
  })
}

const commonParams = [
  `--headers=${options.headers}`,
  `--cert=${options.cert}`,
  `--key=${options.key}`,
  `--pass=${options.pass}`,
  `--ca=${options.ca}`,
  `--tlsAuth=${options.tlsAuth}`,
  `--input-cert=${options['input-cert']}`,
  `--input-key=${options['input-key']}`,
  `--input-pass=${options['input-pass']}`,
  `--input-ca=${options['input-ca']}`,
  `--output-cert=${options['output-cert']}`,
  `--output-key=${options['output-key']}`,
  `--output-pass=${options['output-pass']}`,
  `--output-ca=${options['output-ca']}`,
  `--httpAuthFile=${options.httpAuthFile}`,
  `--concurrency=${options.concurrency}`,
  `--carryoverConcurrencyCount=${options.carryoverConcurrencyCount}`,
  `--intervalCap=${options.intervalCap}`,
  `--concurrencyInterval=${options.concurrencyInterval}`,
  `--overwrite=${options.overwrite}`,
  `--fsCompress=${options.fsCompress}`,
  `--quiet=${options.quiet}`
]

const fileExt = options.fsCompress ? 'json.gz' : 'json'

const validateDirectory = (options, field) => {
  let isDir
  try {
    isDir = fs.lstatSync(options[field]).isDirectory()
  } catch (e) {
    // Handle error
    if (e.code === 'ENOENT') {
      // no such file or directory
      console.error(`Directory --${field} : \`${options[field]}\` does not exists`)
      process.exit(1)
    } else {
      // do something else
    }
  }

  if (!isDir) {
    console.error(`--${field} ${options[field]} is a not directory`)
    process.exit(1)
  }
}

const _fork = (params = [], file = 'elasticdump') => {
  return fork(path.join(__dirname, file), params.concat(commonParams))
}

const attachListeners = (clazz, cb) => {
  clazz.on('close', code => {
    if (code !== 0) {
      if (!options.ignoreChildError) {
        return cb(new Error('CHILD PROCESS EXITED WITH ERROR.  Stopping process'))
      } else {
        return cb()
      }
    } else {
      return cb()
    }
  }).on('error', error => args.log('error', error))
}

if (!options.input) { throw new Error('--input is required') }
if (!options.output) { throw new Error('--output is required') }
args.log('info', `We are performing : ${options.direction}`)
args.log('info', `options: ${JSON.stringify(options)}`)

const matchRegExp = new RegExp(options.match, 'i')
if (options.direction === 'dump') {
  validateDirectory(options, 'output')

  let input = options.input
  if (options.httpAuthFile) {
    input = addAuth(options.input, options.httpAuthFile)
  }
  const inputUrl = new URL(input)
  inputUrl.pathname = '/_aliases'

  const req = {
    url: url.format(inputUrl),
    method: 'GET',
    headers: Object.assign({
      'User-Agent': 'elasticdump',
      'Content-Type': 'application/json'
    }, JSON.parse(options.headers) || {})
  }
  request(req, (err, response) => {
    if (err) {
      args.log('err', err)
      process.exit()
    }
    if (response.statusCode >= 400) {
      args.log('err', `Attempt to list /_aliases failed with status ${response.statusCode} ${response.statusMessage}`)
      process.exit()
    }
    response = JSON.parse(response.body)
    if ('error' in response) {
      args.log('err', response.error.reason)
      process.exit()
    }
    if (!Array.isArray(response)) {
      response = Object.keys(response)
    }
    matchedIndexes = response.filter(index => matchRegExp.test(index))

    dumpWork()
  })
}

if (options.direction === 'load') {
  validateDirectory(options, 'input')
  fs.readdir(options.input, (err, data) => {
    if (err) {
      args.log('error', err)
      throw new Error('Something went wrong reading the list of files')
    }
    // args.log('info', data);
    matchedIndexes = data.map(value => value
      .replace(`.mapping.${fileExt}`, '')
      .replace(`.analyzer.${fileExt}`, '')
      .replace(`.alias.${fileExt}`, '')
      .replace(`.settings.${fileExt}`, '')
      .replace(`.template.${fileExt}`, '')
      .replace(`.${fileExt}`, ''))
      .filter(item => matchRegExp.test(item))
    matchedIndexes = _.uniq(matchedIndexes)
    args.log('info', `list of indexes${JSON.stringify(matchedIndexes)}`)

    loadWork()
  })
}

const dumpWork = () => {
  clearTimeout(workTimeout)
  if (complete === matchedIndexes.length) {
    args.log('info', ' dumping all done ')
    args.log('info', ' bye ')
    process.exit()
  } else if (working === options.parallel) {
    workTimeout = setTimeout(dumpWork, options.interval)
  } else {
    dump()
    workTimeout = setTimeout(dumpWork, options.interval)
  }
}

const loadWork = () => {
  clearTimeout(workTimeout)
  if (complete === matchedIndexes.length) {
    args.log('info', ' indexing all done ')
    args.log('info', ' bye ')
    process.exit()
  } else if (working === options.parallel) {
    workTimeout = setTimeout(loadWork, options.interval)
  } else {
    load()
    workTimeout = setTimeout(loadWork, options.interval)
  }
}

const dump = () => {
  working++
  const index = matchedIndexes[indexCounter]

  if (!index) {
    working--
    return
  }

  indexCounter++

  const input = `${options.input}/${encodeURIComponent(index).toLowerCase()}`
  const outputData = `${options.output}/${index}.${fileExt}`
  const outputMapping = `${options.output}/${index}.mapping.${fileExt}`
  const outputAnalyzer = `${options.output}/${index}.analyzer.${fileExt}`
  const outputAlias = `${options.output}/${index}.alias.${fileExt}`
  const outputSettings = `${options.output}/${index}.settings.${fileExt}`
  const outputTemplate = `${options.output}/${index}.template.${fileExt}`
  const jobs = []

  jobs.push(done => {
    if (options.ignoreTemplate) return done()
    args.log('info', `dumping ${input} to ${outputTemplate}`)

    const templateChild = _fork([
      '--type=template',
      `--input=${input}`,
      `--output=${outputTemplate}`
    ])

    attachListeners(templateChild, done)
  })

  jobs.push(done => {
    if (options.ignoreSettings) return done()
    args.log('info', `dumping ${input} to ${outputSettings}`)

    const settingsChild = _fork([
      '--type=settings',
      `--input=${input}`,
      `--output=${outputSettings}`
    ])

    attachListeners(settingsChild, done)
  })

  jobs.push(done => {
    if (options.ignoreMapping) return done()
    args.log('info', `dumping ${input} to ${outputMapping}`)

    const mappingChild = _fork([
      '--type=mapping',
      `--input=${input}`,
      `--output=${outputMapping}`
    ])

    attachListeners(mappingChild, done)
  })

  jobs.push(done => {
    if (options.ignoreAnalyzer) return done()
    args.log('info', `analyzer ${input} to ${outputAnalyzer}`)

    const analyzerChild = _fork([
      '--type=analyzer',
      `--input=${input}`,
      `--output=${outputAnalyzer}`
    ])

    attachListeners(analyzerChild, done)
  })

  jobs.push(done => {
    if (options.ignoreAlias) return done()
    args.log('info', `analyzer ${input} to ${outputAlias}`)

    const aliasChild = _fork([
      '--type=alias',
      `--input=${input}`,
      `--output=${outputAlias}`
    ])

    attachListeners(aliasChild, done)
  })

  jobs.push(done => {
    if (options.ignoreData) return done()
    args.log('info', `dumping ${input} to ${outputData}`)

    let _transform = []

    if (options.transform) {
      _transform = _.chain(options.transform)
        .castArray()
        .filter(_.negate(_.isEmpty))
        .map(t => {
          return `--transform=${t}`
        })
        .value()
    }

    const dataChild = _fork([
      '--type=data',
      `--input=${input}`,
      `--output=${outputData}`,
      `--scrollId=${options.scrollId}`,
      `--scrollTime=${options.scrollTime}`,
      `--limit=${options.limit}`,
      `--offset=${options.offset}`,
      `--searchBody=${options.searchBody}`,
      `--searchWithTemplate=${options.searchWithTemplate}`,
      `--prefix=${options.prefix}`,
      `--suffix=${options.suffix}`,
      `--support-big-int=${options['support-big-int']}`,
      `--big-int-fields=${options['big-int-fields']}`
    ].concat(_transform))

    attachListeners(dataChild, done)
  })

  async.series(jobs, error => {
    if (error) {
      args.log('error', error)
      process.exit(1)
    } else {
      working--
      complete++
    }
  })
}

const load = () => {
  working++
  const index = matchedIndexes[indexCounter]

  if (!index) {
    working--
    return
  }

  args.log('info', `Working on ${index}`)

  indexCounter++

  const output = `${options.output}/${encodeURIComponent(index).toLowerCase()}`
  const inputData = `${options.input}/${index}.${fileExt}`
  const inputMapping = `${options.input}/${index}.mapping.${fileExt}`
  const inputAnalyzer = `${options.input}/${index}.analyzer.${fileExt}`
  const inputAlias = `${options.input}/${index}.alias.${fileExt}`
  const inputSettings = `${options.input}/${index}.settings.${fileExt}`
  const inputTemplate = `${options.input}/${index}.template.${fileExt}`

  const jobs = []

  jobs.push(done => {
    if (options.ignoreTemplate) return done()
    args.log('info', `indexing template ${inputTemplate} to ${output}`)

    const templateChild = _fork([
      '--type=template',
      `--input=${inputTemplate}`,
      `--output=${output}`
    ])

    templateChild.on('close', code => {
      if (code !== 0) {
        return done(new Error('CHILD PROCESS EXITED WITH ERROR.  Stopping process'))
      } else {
        return done()
      }
    }).on('error', error => args.log('error', error))
  })

  jobs.push(done => {
    if (options.ignoreSettings) return done()
    args.log('info', `indexing settings ${inputSettings} to ${output}`)

    const settingsChild = _fork([
      '--type=settings',
      `--input=${inputSettings}`,
      `--output=${output}`
    ])

    attachListeners(settingsChild, done)
  })

  jobs.push(done => {
    if (options.ignoreAnalyzer) return done()
    args.log('info', `indexing analyzer ${inputAnalyzer} to ${output}`)

    const analyzerChild = _fork([
      '--type=analyzer',
      `--input=${inputAnalyzer}`,
      `--output=${output}`
    ])

    attachListeners(analyzerChild, done)
  })

  jobs.push(done => {
    if (options.ignoreMapping) return done()
    args.log('info', `indexing mapping ${inputMapping} to ${output}`)

    const mappingChild = _fork([
      '--type=mapping',
      `--input=${inputMapping}`,
      `--output=${output}`
    ])

    attachListeners(mappingChild, done)
  })

  jobs.push(done => {
    if (options.ignoreAlias) return done()
    args.log('info', `indexing alias ${inputAlias} to ${output}`)

    const aliasChild = _fork([
      '--type=alias',
      `--input=${inputAlias}`,
      `--output=${output}`
    ])

    attachListeners(aliasChild, done)
  })

  jobs.push(done => {
    if (options.ignoreData) return done()
    args.log('info', `indexing data ${inputData} to ${output}`)

    let _transform = []

    if (options.transform) {
      _transform = _.chain(options.transform)
        .castArray()
        .filter(_.negate(_.isEmpty))
        .map(t => {
          return `--transform=${t}`
        })
        .value()
    }

    const dataChild = _fork([
      '--type=data',
      `--input=${inputData}`,
      `--output=${output}`,
      `--timeout=${options.timeout}`,
      `--limit=${options.limit}`,
      `--offset=${options.offset}`,
      `--prefix=${options.prefix}`,
      `--suffix=${options.suffix}`,
      `--support-big-int=${options['support-big-int']}`,
      `--big-int-fields=${options['big-int-fields']}`
    ].concat(_transform))

    attachListeners(dataChild, done)
  })

  async.series(jobs, error => {
    if (error) {
      args.log('error', error)
      process.exit(1)
    } else {
      working--
      complete++
    }
  })
}
